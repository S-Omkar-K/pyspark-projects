{"cells": [{"cell_type": "code", "execution_count": 1, "id": "a1b249c9", "metadata": {}, "outputs": [{"ename": "ModuleNotFoundError", "evalue": "No module named 'wordcloud'", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IntegerType, StringType\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functions \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"]}], "source": "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec, StringIndexer, IndexToString, Tokenizer\nfrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder \nfrom pyspark.sql.types import IntegerType, StringType\nfrom pyspark.sql import functions as F\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport datetime"}, {"cell_type": "code", "execution_count": 2, "id": "37f56441", "metadata": {}, "outputs": [], "source": "def displayPartitions(df):\n    #get the number of records by partition\n    num = df.rdd.getNumPartitions()\n    print(\"Total number of Partitions:\", num)\n    df.withColumn(\"partitionId\", F.spark_partition_id())\\\n        .groupBy(\"partitionId\")\\\n        .count()\\\n        .orderBy(F.asc(\"count\"))\\\n        .show(num)\n"}, {"cell_type": "code", "execution_count": 3, "id": "09462664", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ":: loading settings :: url = jar:file:/usr/lib/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"}, {"name": "stderr", "output_type": "stream", "text": "Ivy Default Cache set to: /root/.ivy2/cache\nThe jars for the packages stored in: /root/.ivy2/jars\ncom.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\ngraphframes#graphframes added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent-666281f9-0339-4af9-a656-e6945de17585;1.0\n\tconfs: [default]\n\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;4.4.0 in central\n\tfound com.typesafe#config;1.4.2 in central\n\tfound org.rocksdb#rocksdbjni;6.29.5 in central\n\tfound com.amazonaws#aws-java-sdk-bundle;1.11.828 in central\n\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n\tfound com.google.code.gson#gson;2.3 in central\n\tfound it.unimi.dsi#fastutil;7.0.12 in central\n\tfound org.projectlombok#lombok;1.16.8 in central\n\tfound com.google.cloud#google-cloud-storage;2.16.0 in central\n\tfound com.google.guava#guava;31.1-jre in central\n\tfound com.google.guava#failureaccess;1.0.1 in central\n\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n\tfound com.google.errorprone#error_prone_annotations;2.16 in central\n\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n\tfound com.google.http-client#google-http-client;1.42.3 in central\n\tfound io.opencensus#opencensus-contrib-http-util;0.31.1 in central\n\tfound com.google.http-client#google-http-client-jackson2;1.42.3 in central\n\tfound com.google.http-client#google-http-client-gson;1.42.3 in central\n\tfound com.google.api-client#google-api-client;2.1.1 in central\n\tfound commons-codec#commons-codec;1.15 in central\n\tfound com.google.oauth-client#google-oauth-client;1.34.1 in central\n\tfound com.google.http-client#google-http-client-apache-v2;1.42.3 in central\n\tfound com.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 in central\n\tfound com.google.code.gson#gson;2.10 in central\n\tfound com.google.cloud#google-cloud-core;2.9.0 in central\n\tfound com.google.auto.value#auto-value-annotations;1.10.1 in central\n\tfound com.google.cloud#google-cloud-core-http;2.9.0 in central\n\tfound com.google.http-client#google-http-client-appengine;1.42.3 in central\n\tfound com.google.api#gax-httpjson;0.105.1 in central\n\tfound com.google.cloud#google-cloud-core-grpc;2.9.0 in central\n\tfound io.grpc#grpc-core;1.51.0 in central\n\tfound com.google.api#gax;2.20.1 in central\n\tfound com.google.api#gax-grpc;2.20.1 in central\n\tfound io.grpc#grpc-alts;1.51.0 in central\n\tfound io.grpc#grpc-grpclb;1.51.0 in central\n\tfound org.conscrypt#conscrypt-openjdk-uber;2.5.2 in central\n\tfound io.grpc#grpc-protobuf;1.51.0 in central\n\tfound com.google.auth#google-auth-library-credentials;1.13.0 in central\n\tfound com.google.auth#google-auth-library-oauth2-http;1.13.0 in central\n\tfound com.google.api#api-common;2.2.2 in central\n\tfound javax.annotation#javax.annotation-api;1.3.2 in central\n\tfound io.opencensus#opencensus-api;0.31.1 in central\n\tfound io.grpc#grpc-context;1.51.0 in central\n\tfound com.google.api.grpc#proto-google-iam-v1;1.6.22 in central\n\tfound com.google.protobuf#protobuf-java;3.21.10 in central\n\tfound com.google.protobuf#protobuf-java-util;3.21.10 in central\n\tfound com.google.api.grpc#proto-google-common-protos;2.11.0 in central\n\tfound org.threeten#threetenbp;1.6.4 in central\n\tfound com.google.api.grpc#proto-google-cloud-storage-v2;2.16.0-alpha in central\n\tfound com.google.api.grpc#grpc-google-cloud-storage-v2;2.16.0-alpha in central\n\tfound com.google.api.grpc#gapic-google-cloud-storage-v2;2.16.0-alpha in central\n\tfound com.fasterxml.jackson.core#jackson-core;2.14.1 in central\n\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n\tfound io.grpc#grpc-api;1.51.0 in central\n\tfound io.grpc#grpc-auth;1.51.0 in central\n\tfound io.grpc#grpc-stub;1.51.0 in central\n\tfound org.checkerframework#checker-qual;3.28.0 in central\n\tfound com.google.api.grpc#grpc-google-iam-v1;1.6.22 in central\n\tfound io.grpc#grpc-protobuf-lite;1.51.0 in central\n\tfound com.google.android#annotations;4.1.1.4 in central\n\tfound org.codehaus.mojo#animal-sniffer-annotations;1.22 in central\n\tfound io.grpc#grpc-netty-shaded;1.51.0 in central\n\tfound io.perfmark#perfmark-api;0.26.0 in central\n\tfound io.grpc#grpc-googleapis;1.51.0 in central\n\tfound io.grpc#grpc-xds;1.51.0 in central\n\tfound io.opencensus#opencensus-proto;0.2.0 in central\n\tfound io.grpc#grpc-services;1.51.0 in central\n\tfound com.google.re2j#re2j;1.6 in central\n\tfound com.navigamez#greex;1.0 in central\n\tfound dk.brics.automaton#automaton;1.11-8 in central\n\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 in central\n\tfound graphframes#graphframes;0.8.2-spark3.1-s_2.12 in spark-packages\n\tfound org.slf4j#slf4j-api;1.7.16 in central\n:: resolution report :: resolve 1782ms :: artifacts dl 47ms\n\t:: modules in use:\n\tcom.amazonaws#aws-java-sdk-bundle;1.11.828 from central in [default]\n\tcom.fasterxml.jackson.core#jackson-core;2.14.1 from central in [default]\n\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n\tcom.google.android#annotations;4.1.1.4 from central in [default]\n\tcom.google.api#api-common;2.2.2 from central in [default]\n\tcom.google.api#gax;2.20.1 from central in [default]\n\tcom.google.api#gax-grpc;2.20.1 from central in [default]\n\tcom.google.api#gax-httpjson;0.105.1 from central in [default]\n\tcom.google.api-client#google-api-client;2.1.1 from central in [default]\n\tcom.google.api.grpc#gapic-google-cloud-storage-v2;2.16.0-alpha from central in [default]\n\tcom.google.api.grpc#grpc-google-cloud-storage-v2;2.16.0-alpha from central in [default]\n\tcom.google.api.grpc#grpc-google-iam-v1;1.6.22 from central in [default]\n\tcom.google.api.grpc#proto-google-cloud-storage-v2;2.16.0-alpha from central in [default]\n\tcom.google.api.grpc#proto-google-common-protos;2.11.0 from central in [default]\n\tcom.google.api.grpc#proto-google-iam-v1;1.6.22 from central in [default]\n\tcom.google.apis#google-api-services-storage;v1-rev20220705-2.0.0 from central in [default]\n\tcom.google.auth#google-auth-library-credentials;1.13.0 from central in [default]\n\tcom.google.auth#google-auth-library-oauth2-http;1.13.0 from central in [default]\n\tcom.google.auto.value#auto-value-annotations;1.10.1 from central in [default]\n\tcom.google.cloud#google-cloud-core;2.9.0 from central in [default]\n\tcom.google.cloud#google-cloud-core-grpc;2.9.0 from central in [default]\n\tcom.google.cloud#google-cloud-core-http;2.9.0 from central in [default]\n\tcom.google.cloud#google-cloud-storage;2.16.0 from central in [default]\n\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n\tcom.google.code.gson#gson;2.10 from central in [default]\n\tcom.google.errorprone#error_prone_annotations;2.16 from central in [default]\n\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n\tcom.google.guava#guava;31.1-jre from central in [default]\n\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n\tcom.google.http-client#google-http-client;1.42.3 from central in [default]\n\tcom.google.http-client#google-http-client-apache-v2;1.42.3 from central in [default]\n\tcom.google.http-client#google-http-client-appengine;1.42.3 from central in [default]\n\tcom.google.http-client#google-http-client-gson;1.42.3 from central in [default]\n\tcom.google.http-client#google-http-client-jackson2;1.42.3 from central in [default]\n\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n\tcom.google.oauth-client#google-oauth-client;1.34.1 from central in [default]\n\tcom.google.protobuf#protobuf-java;3.21.10 from central in [default]\n\tcom.google.protobuf#protobuf-java-util;3.21.10 from central in [default]\n\tcom.google.re2j#re2j;1.6 from central in [default]\n\tcom.johnsnowlabs.nlp#spark-nlp_2.12;4.4.0 from central in [default]\n\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.4.4 from central in [default]\n\tcom.navigamez#greex;1.0 from central in [default]\n\tcom.typesafe#config;1.4.2 from central in [default]\n\tcommons-codec#commons-codec;1.15 from central in [default]\n\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n\tgraphframes#graphframes;0.8.2-spark3.1-s_2.12 from spark-packages in [default]\n\tio.grpc#grpc-alts;1.51.0 from central in [default]\n\tio.grpc#grpc-api;1.51.0 from central in [default]\n\tio.grpc#grpc-auth;1.51.0 from central in [default]\n\tio.grpc#grpc-context;1.51.0 from central in [default]\n\tio.grpc#grpc-core;1.51.0 from central in [default]\n\tio.grpc#grpc-googleapis;1.51.0 from central in [default]\n\tio.grpc#grpc-grpclb;1.51.0 from central in [default]\n\tio.grpc#grpc-netty-shaded;1.51.0 from central in [default]\n\tio.grpc#grpc-protobuf;1.51.0 from central in [default]\n\tio.grpc#grpc-protobuf-lite;1.51.0 from central in [default]\n\tio.grpc#grpc-services;1.51.0 from central in [default]\n\tio.grpc#grpc-stub;1.51.0 from central in [default]\n\tio.grpc#grpc-xds;1.51.0 from central in [default]\n\tio.opencensus#opencensus-api;0.31.1 from central in [default]\n\tio.opencensus#opencensus-contrib-http-util;0.31.1 from central in [default]\n\tio.opencensus#opencensus-proto;0.2.0 from central in [default]\n\tio.perfmark#perfmark-api;0.26.0 from central in [default]\n\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n\tjavax.annotation#javax.annotation-api;1.3.2 from central in [default]\n\torg.checkerframework#checker-qual;3.28.0 from central in [default]\n\torg.codehaus.mojo#animal-sniffer-annotations;1.22 from central in [default]\n\torg.conscrypt#conscrypt-openjdk-uber;2.5.2 from central in [default]\n\torg.projectlombok#lombok;1.16.8 from central in [default]\n\torg.rocksdb#rocksdbjni;6.29.5 from central in [default]\n\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n\torg.threeten#threetenbp;1.6.4 from central in [default]\n\t:: evicted modules:\n\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 by [com.google.protobuf#protobuf-java-util;3.21.10] in [default]\n\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 by [com.google.protobuf#protobuf-java;3.21.10] in [default]\n\tcom.google.code.gson#gson;2.3 by [com.google.code.gson#gson;2.10] in [default]\n\t---------------------------------------------------------------------\n\t|                  |            modules            ||   artifacts   |\n\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n\t---------------------------------------------------------------------\n\t|      default     |   75  |   0   |   0   |   3   ||   72  |   0   |\n\t---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent-666281f9-0339-4af9-a656-e6945de17585\n\tconfs: [default]\n\t72 artifacts copied, 0 already retrieved (614853kB/877ms)\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n23/05/22 13:56:07 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n23/05/22 13:56:07 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n23/05/22 13:56:08 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n23/05/22 13:56:08 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.johnsnowlabs.nlp_spark-nlp_2.12-4.4.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/graphframes_graphframes-0.8.2-spark3.1-s_2.12.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.typesafe_config-1.4.2.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.rocksdb_rocksdbjni-6.29.5.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.828.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.github.universal-automata_liblevenshtein-3.0.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.cloud_google-cloud-storage-2.16.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.navigamez_greex-1.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.johnsnowlabs.nlp_tensorflow-cpu_2.12-0.4.4.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/it.unimi.dsi_fastutil-7.0.12.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.projectlombok_lombok-1.16.8.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.guava_guava-31.1-jre.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.guava_failureaccess-1.0.1.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.guava_listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.errorprone_error_prone_annotations-2.16.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.j2objc_j2objc-annotations-1.3.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.http-client_google-http-client-1.42.3.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.opencensus_opencensus-contrib-http-util-0.31.1.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.http-client_google-http-client-jackson2-1.42.3.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.http-client_google-http-client-gson-1.42.3.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api-client_google-api-client-2.1.1.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/commons-codec_commons-codec-1.15.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.oauth-client_google-oauth-client-1.34.1.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.http-client_google-http-client-apache-v2-1.42.3.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.apis_google-api-services-storage-v1-rev20220705-2.0.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.code.gson_gson-2.10.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.cloud_google-cloud-core-2.9.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.auto.value_auto-value-annotations-1.10.1.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.cloud_google-cloud-core-http-2.9.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.http-client_google-http-client-appengine-1.42.3.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api_gax-httpjson-0.105.1.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.cloud_google-cloud-core-grpc-2.9.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-core-1.51.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api_gax-2.20.1.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api_gax-grpc-2.20.1.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-alts-1.51.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-grpclb-1.51.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.conscrypt_conscrypt-openjdk-uber-2.5.2.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-protobuf-1.51.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.auth_google-auth-library-credentials-1.13.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.auth_google-auth-library-oauth2-http-1.13.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api_api-common-2.2.2.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/javax.annotation_javax.annotation-api-1.3.2.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.opencensus_opencensus-api-0.31.1.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-context-1.51.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api.grpc_proto-google-iam-v1-1.6.22.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.protobuf_protobuf-java-3.21.10.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.protobuf_protobuf-java-util-3.21.10.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api.grpc_proto-google-common-protos-2.11.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.threeten_threetenbp-1.6.4.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api.grpc_proto-google-cloud-storage-v2-2.16.0-alpha.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api.grpc_grpc-google-cloud-storage-v2-2.16.0-alpha.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api.grpc_gapic-google-cloud-storage-v2-2.16.0-alpha.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.fasterxml.jackson.core_jackson-core-2.14.1.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.2.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-api-1.51.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-auth-1.51.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-stub-1.51.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.checkerframework_checker-qual-3.28.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.api.grpc_grpc-google-iam-v1-1.6.22.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-protobuf-lite-1.51.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.android_annotations-4.1.1.4.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.codehaus.mojo_animal-sniffer-annotations-1.22.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-netty-shaded-1.51.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.perfmark_perfmark-api-0.26.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-googleapis-1.51.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-xds-1.51.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.opencensus_opencensus-proto-0.2.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/io.grpc_grpc-services-1.51.0.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/com.google.re2j_re2j-1.6.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/dk.brics.automaton_automaton-1.11-8.jar added multiple times to distributed cache.\n23/05/22 13:56:15 WARN org.apache.spark.deploy.yarn.Client: Same path resource file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.16.jar added multiple times to distributed cache.\n"}], "source": "spark = SparkSession.builder.appName('Spark-Tweet-Sentiment-Analysis').getOrCreate()"}, {"cell_type": "code", "execution_count": 4, "id": "e920bb80", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/05/22 13:57:50 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n[Stage 2:======================================================>  (78 + 3) / 81]\r"}, {"name": "stdout", "output_type": "stream", "text": "Reading 1442293 tweets in total.\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "#df = spark.read.json(\"gs://dataproc-staging-us-central1-575311154882-olmcvswv/notebooks/jupyter/\")\n\ndf = spark.read.json(\"gs://bigdatafinaldatakira/covid  tweets/\")\nprint(f\"Reading {df.count()} tweets in total.\")"}, {"cell_type": "markdown", "id": "4f7bb42c", "metadata": {}, "source": "## Word Cloud Generate"}, {"cell_type": "code", "execution_count": 5, "id": "04dd7f94", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+\n|                text|\n+--------------------+\n|RT @BPEricAdams: ...|\n|RT @realDonaldTru...|\n|Hugging during Co...|\n|RT @NPR: Maryland...|\n|RT @DrW0mbat: Cor...|\n+--------------------+\nonly showing top 5 rows\n\n"}], "source": "twitter_texts = df.select(\"text\")\ntwitter_texts.show(5)"}, {"cell_type": "code", "execution_count": 6, "id": "d512d75b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+\n|                text|\n+--------------------+\n|the  is an integr...|\n|the fake news kno...|\n|hugging during co...|\n|maryland is repor...|\n|coronavirus in sc...|\n+--------------------+\nonly showing top 5 rows\n\n"}], "source": "twitter_texts = twitter_texts.na.drop(subset=[\"text\"])\n\n# delete @mentions\ntwitter_texts = twitter_texts.withColumn('text', F.regexp_replace(F.col('text'), '@\\S+', ''))\n\n# delete RTs\ntwitter_texts = twitter_texts.withColumn('text', F.regexp_replace(F.col('text'), 'RT', ''))\n\n# delete amp\ntwitter_texts = twitter_texts.withColumn('text', F.regexp_replace(F.col('text'), 'amp', ''))\n\n# tolower\ntwitter_texts = twitter_texts.withColumn(\"text\", F.lower(F.col(\"text\")))\n\n\n\n# Keep only alpha & num\ntwitter_texts = twitter_texts.withColumn(\"text\", F.regexp_replace(F.col(\"text\"), \"[^a-zA-Z0-9\\s]\", \"\"))\n\n# Remove unnecessary spaces\ntwitter_texts = twitter_texts.withColumn(\"text\", F.trim(F.col(\"text\")))\n\n# Remove empty text\ntwitter_texts = twitter_texts.filter(F.length(F.trim(twitter_texts.text)) > 0)\n\n# Show\ntwitter_texts.show(5)"}, {"cell_type": "code", "execution_count": 7, "id": "d7969541", "metadata": {}, "outputs": [], "source": "# Tokenization\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n\n# Stop words removal\nremover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")"}, {"cell_type": "code", "execution_count": 8, "id": "9509240e", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 7:>                                                          (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+\n|            filtered|\n+--------------------+\n|[, integral, comp...|\n|[fake, news, know...|\n|[hugging, corona,...|\n|[maryland, report...|\n|[coronavirus, sco...|\n+--------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "word_cloud_pipeline = Pipeline(stages=[tokenizer, remover])\ntwitter_texts = word_cloud_pipeline.fit(twitter_texts).transform(twitter_texts)\ntwitter_texts = twitter_texts.select(\"filtered\")\ntwitter_texts.show(5)"}, {"cell_type": "code", "execution_count": 9, "id": "50215f4c", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filtered</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[, integral, component, transportation, networ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[fake, news, knows, thanks, katie, httpstcobub...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[hugging, corona, httpstcogijgmnahl3]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[maryland, reporting, 1784, newly, confirmed, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[coronavirus, scotland, garden, centres, plead...</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "                                            filtered\n0  [, integral, component, transportation, networ...\n1  [fake, news, knows, thanks, katie, httpstcobub...\n2              [hugging, corona, httpstcogijgmnahl3]\n3  [maryland, reporting, 1784, newly, confirmed, ...\n4  [coronavirus, scotland, garden, centres, plead..."}, "execution_count": 9, "metadata": {}, "output_type": "execute_result"}], "source": "# convert to pandas DataFrame for word cloud generation\npandas_df = twitter_texts.toPandas()\npandas_df.head()"}, {"cell_type": "code", "execution_count": null, "id": "93d272bb", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Reading chunk 0\nReading chunk 1\nReading chunk 2\nReading chunk 3\nReading chunk 4\nReading chunk 5\nReading chunk 6\nReading chunk 7\n"}], "source": "from sklearn.feature_extraction.text import CountVectorizer\n# Initialize an empty dictionary to store the total word frequencies\ntotal_frequencies = {}\n\n# Initialize a CountVectorizer to extract word frequencies\nvectorizer = CountVectorizer()\ni = 0\n# Loop over chunks of the DataFrame\nfor chunk in np.array_split(pandas_df, 10):\n    print(f\"Reading chunk {i}\")\n    # Adjust number of chunks as per memory capacity\n    # Get the text from this chunk\n    text = ' '.join([' '.join(row.filtered) for index, row in chunk.iterrows()])\n    \n    # Calculate word frequencies for this chunk\n    frequencies = vectorizer.fit_transform([text]).toarray().ravel()\n    words = vectorizer.get_feature_names()\n\n    # Add the word frequencies of this chunk to the total frequencies\n    for word, freq in zip(words, frequencies):\n        total_frequencies[word] = total_frequencies.get(word, 0) + freq\n    \n    i += 1\n\n# Generate the wordcloud\nwordcloud = WordCloud(width = 1000, height = 800).generate_from_frequencies(total_frequencies)\n\n# plot the WordCloud image                       \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show()  \n"}, {"cell_type": "markdown", "id": "daf979b6", "metadata": {}, "source": "## Sentiment Analysis"}, {"cell_type": "code", "execution_count": 5, "id": "ef7ed44b", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+----+---------+--------------------+\n|  id|sentiment|                text|\n+----+---------+--------------------+\n|3204|      sad|agree the poor in...|\n|1431|      joy|if only i could h...|\n| 654|      joy|will nature conse...|\n|2530|      sad|\"coronavirus disa...|\n|2296|      sad|uk records lowest...|\n+----+---------+--------------------+\nonly showing top 5 rows\n\n"}], "source": "df_labeled_data = spark.read.csv(\"gs://bigdatafinaldatakira/Sentimentdata.csv\"\n                         ,header=True, inferSchema=True)\ndf_labeled_data.show(5)"}, {"cell_type": "code", "execution_count": 6, "id": "35492428", "metadata": {}, "outputs": [], "source": "df_labeled_data = df_labeled_data.cache()"}, {"cell_type": "code", "execution_count": 7, "id": "bb70b37e", "metadata": {}, "outputs": [{"data": {"text/plain": "3090"}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": "df_labeled_data.count()"}, {"cell_type": "code", "execution_count": 8, "id": "9f773cc9", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Total number of Partitions: 1\n+-----------+-----+\n|partitionId|count|\n+-----------+-----+\n|          0| 3090|\n+-----------+-----+\n\n"}], "source": "displayPartitions(df_labeled_data)"}, {"cell_type": "code", "execution_count": 9, "id": "627b4c0d", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- id: integer (nullable = true)\n |-- sentiment: string (nullable = true)\n |-- text: string (nullable = true)\n\n"}], "source": "df_labeled_data.printSchema()"}, {"cell_type": "code", "execution_count": 10, "id": "4fbd25b8", "metadata": {}, "outputs": [], "source": "# Tokenization\ntokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n\n# Stop words removal\nremover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n\n# Word2Vec\nword2Vec = Word2Vec(vectorSize=100, minCount=2, inputCol=\"filtered\", outputCol=\"features\")\n\n# Indexer\nindexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")"}, {"cell_type": "code", "execution_count": 11, "id": "472cd879", "metadata": {"scrolled": true}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/05/22 13:59:47 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n23/05/22 13:59:47 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"}, {"name": "stdout", "output_type": "stream", "text": "+----+---------+--------------------+--------------------+--------------------+--------------------+-----+\n|  id|sentiment|                text|               words|            filtered|            features|label|\n+----+---------+--------------------+--------------------+--------------------+--------------------+-----+\n|3204|      sad|agree the poor in...|[agree, the, poor...|[agree, poor, ind...|[-0.0123396027047...|  1.0|\n|1431|      joy|if only i could h...|[if, only, i, cou...|[spent, cutie, vc...|[-0.0019741141702...|  3.0|\n| 654|      joy|will nature conse...|[will, nature, co...|[nature, conserva...|[-0.0118956502733...|  3.0|\n|2530|      sad|\"coronavirus disa...|[coronavirus, dis...|[coronavirus, dis...|[-0.0166720755223...|  1.0|\n|2296|      sad|uk records lowest...|[uk, records, low...|[uk, records, low...|[-0.0230484596368...|  1.0|\n+----+---------+--------------------+--------------------+--------------------+--------------------+-----+\nonly showing top 5 rows\n\n"}], "source": "# Build the NLP preprocessing pipeline\nnlp_pipeline = Pipeline(stages=[tokenizer, remover, word2Vec, indexer])\nnlp_model = nlp_pipeline.fit(df_labeled_data)\nprocessed_data = nlp_model.transform(df_labeled_data)\nprocessed_data.show(5)"}, {"cell_type": "code", "execution_count": 12, "id": "1e7693b0", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "lr = LogisticRegression()\n\n# Build the classification pipeline using Logistic Regression\npipeline = Pipeline(stages=[lr])\n\n# Find the best hyperparameter\nparamGrid = ParamGridBuilder() \\\n    .addGrid(lr.regParam, [0.001, 0.01, 0.05, 0.1]) \\\n    .addGrid(lr.maxIter, [30, 50, 100])\\\n    .build()\n\n# Do cross validation\ncrossval = CrossValidator(estimator = pipeline,\n                          estimatorParamMaps = paramGrid,\n                          evaluator = MulticlassClassificationEvaluator(),\n                          numFolds = 5)  # use 5 folds\n\n# Split data into train and test\ntrain, test = processed_data.randomSplit([0.8, 0.2], seed=13)\n\n# Run cross-validation, and choose the best set of parameters.\nlrCvModel = crossval.fit(train)"}, {"cell_type": "code", "execution_count": 13, "id": "ef14efb8", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Test set accuracy = 0.5380794701986755\nBest regularization parameter:  0.001\n"}], "source": "predictions = lrCvModel.transform(test)\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test set accuracy = \" + str(accuracy))\n\n# Get the best model\nbestModel = lrCvModel.bestModel\n\n# Assuming the LogisticRegression stage is the last one in the pipeline\nbest_lr_model = bestModel.stages[-1]  \n\nprint(\"Best regularization parameter: \", best_lr_model._java_obj.getRegParam())"}, {"cell_type": "code", "execution_count": 14, "id": "40905ad2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------------+-------------+--------------+--------------------+\n|          created_at|                 id|retweet_count|favorite_count|                text|\n+--------------------+-------------------+-------------+--------------+--------------------+\n|Wed Jul 01 15:10:...|1278345250428370944|            0|             1|@davidmweissman Y...|\n|Wed Jul 01 15:10:...|1278345250684182528|           94|             0|RT @AdoptionsUk: ...|\n|Wed Jul 01 15:10:...|1278345250071666688|         3700|             0|RT @davidplouffe:...|\n|Wed Jul 01 15:10:...|1278345250549923840|        93417|             0|RT @lookitstaylor...|\n|Wed Jul 01 15:10:...|1278345249782239233|         4037|             0|RT @RepSwalwell: ...|\n+--------------------+-------------------+-------------+--------------+--------------------+\nonly showing top 5 rows\n\n"}], "source": "columns_to_keep = [\"created_at\", \"id\", \"retweet_count\", \"favorite_count\", \"text\"]\ndf = df.select(*columns_to_keep)\ndf.show(5)"}, {"cell_type": "code", "execution_count": 15, "id": "5886cee0", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------------+-------------+--------------+--------------------+\n|          created_at|                 id|retweet_count|favorite_count|                text|\n+--------------------+-------------------+-------------+--------------+--------------------+\n|Wed Jul 01 15:10:...|1278345250428370944|            0|             1| Yes. I read SPY ...|\n|Wed Jul 01 15:10:...|1278345250684182528|           94|             0|  Happy news Silv...|\n|Wed Jul 01 15:10:...|1278345250071666688|         3700|             0|  COVID-19 may ha...|\n|Wed Jul 01 15:10:...|1278345250549923840|        93417|             0|  Shaming people ...|\n|Wed Jul 01 15:10:...|1278345249782239233|         4037|             0|  I\u2019m so angry wi...|\n+--------------------+-------------------+-------------+--------------+--------------------+\nonly showing top 5 rows\n\n"}], "source": "# delete @mentions\ndf = df.withColumn('text', F.regexp_replace(F.col('text'), '@\\S+', ''))\n\n# delete RTs\ndf = df.withColumn('text', F.regexp_replace(F.col('text'), 'RT', ''))\ndf.show(5)"}, {"cell_type": "code", "execution_count": 16, "id": "34b3ffb3", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------------+-------------+--------------+--------------------+\n|          created_at|                 id|retweet_count|favorite_count|                text|\n+--------------------+-------------------+-------------+--------------+--------------------+\n|Wed Jul 01 15:10:...|1278345250428370944|            0|             1|yes i read spy ma...|\n|Wed Jul 01 15:10:...|1278345250684182528|           94|             0|happy news silver...|\n|Wed Jul 01 15:10:...|1278345250071666688|         3700|             0|covid19 may have ...|\n|Wed Jul 01 15:10:...|1278345250549923840|        93417|             0|shaming people fo...|\n|Wed Jul 01 15:10:...|1278345249782239233|         4037|             0|im so angry with ...|\n+--------------------+-------------------+-------------+--------------+--------------------+\nonly showing top 5 rows\n\n"}], "source": "df = df.withColumn(\"text\", df[\"text\"].cast(StringType()))\n\n# remove na\ndf = df.na.drop(subset=[\"text\"])\n\n# tolower\ndf = df.withColumn(\"text\", F.lower(F.col(\"text\")))\n\n# Keep only alpha & num\ndf = df.withColumn(\"text\", F.regexp_replace(F.col(\"text\"), \"[^a-zA-Z0-9\\s]\", \"\"))\n\n# Remove unnecessary spaces\ndf = df.withColumn(\"text\", F.trim(F.col(\"text\")))\n\n# Remove empty text\ndf = df.filter(F.length(F.trim(df.text)) > 0)\n\n# Show\ndf.show(5)"}, {"cell_type": "code", "execution_count": 17, "id": "f923247b", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------+-------------------+-------------+--------------+--------------------+\n|         created_at|                 id|retweet_count|favorite_count|                text|\n+-------------------+-------------------+-------------+--------------+--------------------+\n|2020-07-01 15:10:32|1278345250428370944|            0|             1|yes i read spy ma...|\n|2020-07-01 15:10:32|1278345250684182528|           94|             0|happy news silver...|\n|2020-07-01 15:10:32|1278345250071666688|         3700|             0|covid19 may have ...|\n|2020-07-01 15:10:32|1278345250549923840|        93417|             0|shaming people fo...|\n|2020-07-01 15:10:32|1278345249782239233|         4037|             0|im so angry with ...|\n+-------------------+-------------------+-------------+--------------+--------------------+\nonly showing top 5 rows\n\n"}], "source": "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n\n\n# first transform created_at column to unix timestamp and then get the correct time_stamp\ndf = df.withColumn(\n    \"created_at\", \n    F.to_timestamp(F.unix_timestamp(df[\"created_at\"], \"EEE MMM dd HH:mm:ss Z yyyy\"))\n)\n\ndf.show(5)"}, {"cell_type": "code", "execution_count": 18, "id": "dbd149fa", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------+-------------------+-------------+--------------+--------------------+\n|         created_at|                 id|retweet_count|favorite_count|                text|\n+-------------------+-------------------+-------------+--------------+--------------------+\n|2020-07-01 15:10:32|1278345250428370944|            0|             1|yes i read spy ma...|\n|2020-07-01 15:10:32|1278345250684182528|           94|             0|happy news silver...|\n|2020-07-01 15:10:32|1278345250071666688|         3700|             0|covid19 may have ...|\n|2020-07-01 15:10:32|1278345250549923840|        93417|             0|shaming people fo...|\n|2020-07-01 15:10:32|1278345249782239233|         4037|             0|im so angry with ...|\n+-------------------+-------------------+-------------+--------------+--------------------+\nonly showing top 5 rows\n\nroot\n |-- created_at: timestamp (nullable = true)\n |-- id: long (nullable = true)\n |-- retweet_count: integer (nullable = true)\n |-- favorite_count: integer (nullable = true)\n |-- text: string (nullable = true)\n\n"}], "source": "df = df.withColumn(\"retweet_count\", df[\"retweet_count\"].cast(IntegerType()))\ndf = df.withColumn(\"favorite_count\", df[\"favorite_count\"].cast(IntegerType()))\ndf.show(5)\ndf.printSchema()"}, {"cell_type": "code", "execution_count": 19, "id": "13e694cc", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- created_at: timestamp (nullable = true)\n |-- id: long (nullable = true)\n |-- retweet_count: integer (nullable = true)\n |-- favorite_count: integer (nullable = true)\n |-- text: string (nullable = true)\n\n"}], "source": "# Check the types\ndf.printSchema()"}, {"cell_type": "code", "execution_count": 20, "id": "43873875", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Create a new nlp pipeline\nmodel_pipeline = Pipeline(stages=[tokenizer, remover, word2Vec, best_lr_model])\n\n# Fit the tweet data\nmodel = model_pipeline.fit(df)"}, {"cell_type": "code", "execution_count": 21, "id": "db3bdf19", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 4055:>                                                       (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------+--------------------+-------------------+\n|         created_at|                text|predicted_sentiment|\n+-------------------+--------------------+-------------------+\n|2020-07-01 15:10:32|yes i read spy ma...|                joy|\n|2020-07-01 15:10:32|happy news silver...|                joy|\n|2020-07-01 15:10:32|covid19 may have ...|                joy|\n|2020-07-01 15:10:32|shaming people fo...|                joy|\n|2020-07-01 15:10:32|im so angry with ...|              anger|\n|2020-07-01 15:10:32|my stepsister who...|                sad|\n|2020-07-01 15:10:32|the idsi strongly...|                joy|\n|2020-07-01 15:10:32|          great news|                joy|\n|2020-07-01 15:10:32|trumps gop allies...|                joy|\n|2020-07-01 15:10:32|aaaaaand this is ...|                joy|\n|2020-07-01 15:10:32|the us represents...|              anger|\n|2020-07-01 15:10:32|much like the pan...|                joy|\n|2020-07-01 15:10:32|give 2000month to...|                joy|\n|2020-07-01 15:10:32|while mates who a...|                joy|\n|2020-07-01 15:10:32|my household hasn...|                sad|\n|2020-07-01 15:10:32|happy to share th...|                joy|\n|2020-07-01 15:10:32|i wrote an entire...|                joy|\n|2020-07-01 15:10:32|rent is due today...|                joy|\n|2020-07-01 15:10:32|do you really thi...|                joy|\n|2020-07-01 15:10:32|something is wron...|                joy|\n+-------------------+--------------------+-------------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Make prediction\npredictions = model.transform(df)\n\n# Convert prediction back to original sentiment\nindexer_model = indexer.fit(df_labeled_data)\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predicted_sentiment\", labels=indexer_model.labels)\n\n# Apply IndexToString to the DataFrame with predictions\nconverted = labelConverter.transform(predictions)\n\n# Select the columns\nfinal_result = converted.select(\"created_at\", \"text\", \"predicted_sentiment\")\n\n# Show results\nfinal_result.show()\n"}, {"cell_type": "code", "execution_count": 22, "id": "98d1edd6", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"ename": "NameError", "evalue": "name 'datetime' is not defined", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "Cell \u001b[0;32mIn[22], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m df_pd\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Filter the DataFrame to include only data on or after 2020-03-25\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m df_pd \u001b[38;5;241m=\u001b[39m df_pd[df_pd\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241m.\u001b[39mdate(\u001b[38;5;241m2020\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m25\u001b[39m)]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Calculate the total number of tweets per day\u001b[39;00m\n\u001b[1;32m     18\u001b[0m total_daily_counts \u001b[38;5;241m=\u001b[39m df_pd\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mreset_index()\n", "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"]}], "source": "final_result = final_result.withColumn(\"date\", F.to_date(F.col(\"created_at\")))\n\n# Count by date & lable\ndaily_counts = final_result.groupBy(\"date\", \"predicted_sentiment\").count()\n\n\n# Get the pandas DataFrame\ndf_pd = daily_counts.orderBy(\"date\").toPandas()\n\n\n# Set the date as index\ndf_pd.set_index(\"date\", inplace=True)\n\n# Filter the DataFrame to include only data on or after 2020-03-25\ndf_pd = df_pd[df_pd.index >= datetime.date(2020, 3, 25)]\n\n# Calculate the total number of tweets per day\ntotal_daily_counts = df_pd.groupby(\"date\")[\"count\"].sum().reset_index()\ntotal_daily_counts.columns = [\"date\", \"total_count\"]\n\n# Merge the total counts back to the original dataframe\ndf_pd = pd.merge(df_pd, total_daily_counts, on=\"date\")\n\n# Calculate the proportion of each sentiment per day\ndf_pd[\"proportion\"] = df_pd[\"count\"] / df_pd[\"total_count\"]\n\n# Set the `date` as index again\ndf_pd.set_index(\"date\", inplace=True)\n\n# Get unique sentiment types\nsentiment_types = df_pd['predicted_sentiment'].unique()\n\n# Plot each sentiment type in the same plot\nplt.figure(figsize=(15,8))\nfor sentiment in sentiment_types:\n    df_sentiment = df_pd[df_pd['predicted_sentiment'] == sentiment]\n    plt.plot(df_sentiment.index, df_sentiment['proportion'], label=sentiment)\n\nplt.xticks(rotation=45)\nplt.legend()\nplt.show()"}, {"cell_type": "code", "execution_count": null, "id": "fa0a2fdf", "metadata": {}, "outputs": [], "source": "# Save the result to the csv file\ncolumns_to_keep = [\"created_at\", \"text\", 'predicted_sentiment']\nfinal_result = final_result.select(*columns_to_keep)\nfinal_result.repartition(1).write.csv('gs://dataproc-staging-us-central1-575311154882-olmcvswv/notebooks/jupyter/output', header=True)"}, {"cell_type": "code", "execution_count": null, "id": "71e49584", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}